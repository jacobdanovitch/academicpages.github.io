class: inverse, center, middle

# Related Work

<hr/>

---

# Past

* *Memory Networks* (Weston et al. 2015)
  * Similar idea, but problem: hard max
  * When you query you select the best key... but then can't backpropagate.

---

# Present

* Dominant tool: transformer
* From *Attention is All You Need* (Vaswani et al. 2017)
    * Recurrent attention -> self attention
    * More efficient computationally, so we can use big datasets.
    * Everything connected in constant operations instead of scaling with number of hops.

---

# Future?

* Example: *Hopfield Networks is All You Need* (Ramsauer et al. 2020)
    * A different memory mechanism.
    * Original Hopfield: store discrete binary pattern using which neurons fire.
      * Here: make it continuous. 
    * Then transformers are a special case! 
      * Query retrieves from Hopfield network.
      
---
      
# Conclusion

- A potentially widely applicable way to learn a memory storage system.
- Related to transformer models.
- Experiments are limited by datasets, but show some improvement.