---
title: "ReMemBERT: Recurrent Memory-augmented BERT for Conversational Text Classification"
excerpt: "Adapting BERT for multi-turn conversations using MAC cells."
collection: publications
permalink: /publication/remembert
# date: 2009-10-01
# venue: 'Journal 1'
# paperurl: 'http://academicpages.github.io/files/paper1.pdf'
citation: "Danovitch, J., & SalahEldeen, H. (2019). <i>ReMemBERT: Recurrent Memory-Augmented BERT for Conversational Text Classification.</i> Submitted to ACL 2020, Seattle, USA."
---

Recommended citation: Your Name, You. (2009). "Paper Title Number 1." <i>Journal 1</i>. 1(1).

Supervised by: [Hany SalahEldeen](https://scholar.google.com/citations?user=XpmZBggAAAAJ&hl=en)

<br/> <i style='font-size: small'>Work primarily completed during internship at Microsoft.</i>

Intelligent assistants (IAs) such as Google Assistant, Alexa, Siri, and Cortana, provide users with a natural language interface to complete many common tasks, such as setting an alarm, sending an email, or playing music, using only a brief utterance  (e.g., ”Alexa, play Despacito.”). To move from isolated utterances to a more natural, conversational setting, IAs must be able to understand each turn of dialog within the greater conversational context.

 To this end, we propose a memory-based adaption of BERT for tasks which involve classifying utterances within a conversation, such as intent, topic, dialog act, or emotion classification, which we unify as the task of 'conversational text classification'. We compare our method to several benchmarks, and show that the use of a conversation-aware model improves performance.

gpihpgih